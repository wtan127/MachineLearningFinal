---
title: "Mobile Activity Prediction"
author: "wtan"
date: "March 25, 2016"
output: html_document
---

###Executive Summary
10 Participants were asked to use dumbbells in 5 different ways (Class A-E), where Class A means that the exercises were performed correctly, and the other classes represent common mistakes. (source: http://groupware.les.inf.puc-rio.br/har).

###Data Processing
```{r echo=TRUE}
library(caret)
library(ggplot2)
library(dplyr)
fileURL1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileURL2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileURL1, destfile = "train.csv")
download.file(fileURL2, destfile= "test.csv")
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

There are 6 people in the train set. There are 20 different time stamps.

###Exploratory Analysis
19622 rows and 160 columns is a lot.
```{r echo=TRUE}
dim(train)
```
looking at str(train), it's clear that there are a bunch of numeric values that are displayed as factors.
```{r echo=TRUE}
qplot(train$classe)
```
looking at classes of both train and test, there are inconsistencies where there are 100 columns of logical values in test that aren't in train. We are only going to use columns where the classes are consistent.
```{r echo=TRUE}
trainclass <- sapply(train, class)
testclass <- sapply(test, class)
table(trainclass)
```
```{r echo=TRUE}
table(testclass)
```
```{r echo=TRUE}
sameclass <- trainclass == testclass
table(sameclass)
```
```{r echo=TRUE}
train2 <- train[ ,sameclass]
test2 <- test[ ,sameclass]
rowhasna <- apply(train2, 1, function(x){any(is.na(x))})
sum(rowhasna)
```
```{r echo=TRUE}
rowhadnatest <- apply(test2, 1, function(x){any(is.na(x))})
sum(rowhadnatest)
```
```{r echo=TRUE}
samename <- names(train2) == names(test2)
sum(samename)
```
Now we can add the classe to the end. For the test set, we assume that the classe is A, which is the null hypothesis.
```{r echo=TRUE}
train2$classe <- train$classe
test2$classe <- as.factor(rep("A", 20))
```

#running the caret packages are too time intensive and keep crashing my computer, so I'm going to narrow down the possible columns further. I ran plots against classe for all the factors and decided on the following.
Many variables don't affect the outcome (like date), and many are mostly blank.
```{r echo=TRUE}
nameindex <- names(train2)
x <- nameindex[c(7,8, 10, 11, 16, 17, 19, 20, 21, 23, 24, 25, 28, 31, 32, 34, 36, 37,42, 46, 47, 48, 53, 56, 57)]
trainsub <- select(train2, c(7,8, 10, 11, 16, 17, 19, 20, 21, 23, 24, 25, 28, 31, 32, 34, 36, 37,42, 46, 47, 48, 53, 56, 57))
testsub <- select(test2, c(7,8, 10, 11, 16, 17, 19, 20, 21, 23, 24, 25, 28, 31, 32, 34, 36, 37,42, 46, 47, 48, 53, 56, 57))
print(x)
```
```{r echo=TRUE}
x2 <- nameindex[c(10, 11, 19, 20, 31, 34, 36, 46, 47, 53, 57)]
trainsub2 <- select(train2, c(10, 11, 19, 20, 31, 34, 36, 46, 47, 53, 57))
testsub2 <- select(test2, c(10, 11, 19, 20, 31, 34, 36, 46, 47, 53, 57))
print(x2)
```
###Method
There are a lot of factors that go into the classe variable. When dealing with the data set, it seems useful to use random forests or boosting methods. I tried to run random forest and gbm boosting with the full dataset, but my very old computer simply cannot handle it. It crashed multiple times times.
  
- Using the new dataset, this should be much faster. We are going to try 3 methods: Tree (rpart), Random Forest (rf), and Boosting (gbm).
- Cross Validation: I used 5-fold cross validation, mostly becuase of the time constraint.
```{r echo=TRUE}
modtree <- train(classe~., method="rpart", data=trainsub, trControl=trainControl(method="cv", number=5))
#print(modtree$finalModel)
predict(modtree, testsub)
```
```{r stillsuperslowrf, echo=TRUE, cache=TRUE}
modrf <- train(classe~., method="rf", data=trainsub2, trControl=trainControl(method="cv", number=5))
predict(modrf, testsub2)
```
gbm is recommended to be used without adjusting the trainControl.
```{r stillsuperslowgbm, echo=TRUE, cache=TRUE}
modgbm <- train(classe~., method="gbm", data=trainsub2, verbose=FALSE)
predict(modgbm, testsub2)
```

###Results
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. 

The expected out of sample error is probably not fairly significant, because a lot of the relationships are close with very wide 25-75%iles. I am guessing that boosting would give the best results.

Reults from quiz:
Tree: 8/20
RF: 20/20




###Appendix

```{r echo=TRUE}
table(train$user_name)
```
```{r echo=TRUE}
table(train$cvtd_timestamp)
```

Example of one of the plots I ran to pick the variables
```{r echo=TRUE}
plot(train2$classe, train2[ ,8])
```

#38, 39, 40, 51, 52 would need to be log10